{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "automated-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "elegant-directive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "from add import *\n",
    "c = mul_nos(3,4)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elk_copy imoprt *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-healing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from drain_copy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class ELKConnector:\n",
    "    \"\"\"\n",
    "    Description: This class is used for querying log data from Elasticsearch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hosts, index_name, log_lines):\n",
    "        self.hosts = hosts\n",
    "        self.index_name = index_name\n",
    "        self.log_lines = log_lines\n",
    "\n",
    "    def log_time(self, message):\n",
    "        print(message + ': ', time.ctime(time.time()))\n",
    "\n",
    "    # Create Elasticsearch client\n",
    "    def client_creating(self, http_auth, scheme, use_ssl, verify_certs, ca_certs, timeout):\n",
    "        \"\"\"\n",
    "        :param http_auth: tuple, the tuple stores http authentication\n",
    "        :param scheme: str, scheme (\"http\" or \"https\")\n",
    "        :param use_ssl: bool, whether use SSL for connection\n",
    "        :param verify_certs: bool, whether need to verify certification\n",
    "        :param ca_certs: str, path of certification\n",
    "        :param timeout: int, timeout\n",
    "        :return:\n",
    "            client: object, Elasticsearch client\n",
    "        \"\"\"\n",
    "        self.log_time(\"Connection handshake start\")\n",
    "        client = Elasticsearch(\n",
    "            [host['host'] for host in self.hosts],\n",
    "            http_auth=http_auth,\n",
    "            scheme=scheme,\n",
    "            use_ssl=use_ssl,\n",
    "            verify_certs=verify_certs,\n",
    "            ca_certs=ca_certs,\n",
    "            timeout=timeout\n",
    "        )\n",
    "        try:\n",
    "            # use the JSON library's dump() method for indentation\n",
    "            info = json.dumps(client.info(), indent=4)\n",
    "            # pass client object to info() method\n",
    "            print(\"Elasticsearch client info():\", info)\n",
    "        except exceptions.ConnectionError as err:\n",
    "            # print ConnectionError for Elasticsearch\n",
    "            print(\"\\nElasticsearch info() ERROR:\", err)\n",
    "            print(\"\\nThe client host:\", self.hosts, \"is invalid or cluster is not running\")\n",
    "            # change the client's value to 'None' if ConnectionError\n",
    "            client = None\n",
    "        self.log_time(\"Connection established\")\n",
    "        return client\n",
    "\n",
    "    # Query log data from Elasticsearch\n",
    "    def log_querying(self, client,\n",
    "                     included_columns=['@timestamp', 'logLevel', 'logMessage'],\n",
    "                     rename_dict=None):\n",
    "        \"\"\"\n",
    "        :param client: object, Elasticsearch client\n",
    "        :param included_columns: list, columns to include in search\n",
    "        :param rename_dict: dict, columns to rename in search\n",
    "        :return:\n",
    "            log_data: dataframe (pandas), contains log data queried from Elasticsearch\n",
    "        \"\"\"\n",
    "        if rename_dict is None:\n",
    "            rename_dict = {'logMessage': ModelConfig.log_message_column,\n",
    "                           '@timestamp': ModelConfig.timestamp_column,\n",
    "                           'logLevel': ModelConfig.log_level_column}\n",
    "\n",
    "        self.log_time(\"Log querying start\")\n",
    "        log_data = pd.DataFrame()\n",
    "        doc_count = 0\n",
    "\n",
    "        query_results = Search(using=client, index=self.index_name).query(\"match\", service=\"core\").params(\n",
    "            preserve_order=True).extra(_source={'includes': included_columns})\n",
    "\n",
    "        for hit in query_results.scan():\n",
    "            log_data = log_data.append(hit.to_dict(), ignore_index=True)\n",
    "            doc_count += 1\n",
    "            if doc_count % 10000 == 0:\n",
    "                print(doc_count)\n",
    "            if doc_count >= self.log_lines:\n",
    "                break\n",
    "\n",
    "        try:\n",
    "            # rename columns\n",
    "            log_data.rename(columns=rename_dict, inplace=True)\n",
    "        except:\n",
    "            print(\"Error in finding \\\"log_message\\\" field\")\n",
    "\n",
    "        # Removing duplicates\n",
    "        log_data = log_data.astype(str).drop_duplicates(subset=[ModelConfig.timestamp_column,\n",
    "                                                                ModelConfig.log_message_column],\n",
    "                                                        keep='first').reset_index(drop=True)\n",
    "\n",
    "        # log_data = log_data[[ModelConfig.timestamp_column,\n",
    "        #                      ModelConfig.log_level_column,\n",
    "        #                      ModelConfig.log_message_column]]\n",
    "\n",
    "        self.log_time(\"Log querying end\")\n",
    "        return log_data\n",
    "        print(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "arbitrary-matter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kukaur/Parsed_Log_Files/Parsed_Drain\n",
      "Parsing: ./BOI-JP-BBUEM1_CORE_20200608_0002_in_use.txt\n",
      "Processed 1.0% of log lines.\n",
      "Processed 2.0% of log lines.\n",
      "Processed 2.9% of log lines.\n",
      "Processed 3.9% of log lines.\n",
      "Processed 4.9% of log lines.\n",
      "Processed 5.9% of log lines.\n",
      "Processed 6.8% of log lines.\n",
      "Processed 7.8% of log lines.\n",
      "Processed 8.8% of log lines.\n",
      "Processed 9.8% of log lines.\n",
      "Processed 10.7% of log lines.\n",
      "Processed 11.7% of log lines.\n",
      "Processed 12.7% of log lines.\n",
      "Processed 13.7% of log lines.\n",
      "Processed 14.6% of log lines.\n",
      "Processed 15.6% of log lines.\n",
      "Processed 16.6% of log lines.\n",
      "Processed 17.6% of log lines.\n",
      "Processed 18.5% of log lines.\n",
      "Processed 19.5% of log lines.\n",
      "Processed 20.5% of log lines.\n",
      "Processed 21.5% of log lines.\n",
      "Processed 22.4% of log lines.\n",
      "Processed 23.4% of log lines.\n",
      "Processed 24.4% of log lines.\n",
      "Processed 25.4% of log lines.\n",
      "Processed 26.4% of log lines.\n",
      "Processed 27.3% of log lines.\n",
      "Processed 28.3% of log lines.\n",
      "Processed 29.3% of log lines.\n",
      "Processed 30.3% of log lines.\n",
      "Processed 31.2% of log lines.\n",
      "Processed 32.2% of log lines.\n",
      "Processed 33.2% of log lines.\n",
      "Processed 34.2% of log lines.\n",
      "Processed 35.1% of log lines.\n",
      "Processed 36.1% of log lines.\n",
      "Processed 37.1% of log lines.\n",
      "Processed 38.1% of log lines.\n",
      "Processed 39.0% of log lines.\n",
      "Processed 40.0% of log lines.\n",
      "Processed 41.0% of log lines.\n",
      "Processed 42.0% of log lines.\n",
      "Processed 42.9% of log lines.\n",
      "Processed 43.9% of log lines.\n",
      "Processed 44.9% of log lines.\n",
      "Processed 45.9% of log lines.\n",
      "Processed 46.8% of log lines.\n",
      "Processed 47.8% of log lines.\n",
      "Processed 48.8% of log lines.\n",
      "Processed 49.8% of log lines.\n",
      "Processed 50.7% of log lines.\n",
      "Processed 51.7% of log lines.\n",
      "Processed 52.7% of log lines.\n",
      "Processed 53.7% of log lines.\n",
      "Processed 54.7% of log lines.\n",
      "Processed 55.6% of log lines.\n",
      "Processed 56.6% of log lines.\n",
      "Processed 57.6% of log lines.\n",
      "Processed 58.6% of log lines.\n",
      "Processed 59.5% of log lines.\n",
      "Processed 60.5% of log lines.\n",
      "Processed 61.5% of log lines.\n",
      "Processed 62.5% of log lines.\n",
      "Processed 63.4% of log lines.\n",
      "Processed 64.4% of log lines.\n",
      "Processed 65.4% of log lines.\n",
      "Processed 66.4% of log lines.\n",
      "Processed 67.3% of log lines.\n",
      "Processed 68.3% of log lines.\n",
      "Processed 69.3% of log lines.\n",
      "Processed 70.3% of log lines.\n",
      "Processed 71.2% of log lines.\n",
      "Processed 72.2% of log lines.\n",
      "Processed 73.2% of log lines.\n",
      "Processed 74.2% of log lines.\n",
      "Processed 75.1% of log lines.\n",
      "Processed 76.1% of log lines.\n",
      "Processed 77.1% of log lines.\n",
      "Processed 78.1% of log lines.\n",
      "Processed 79.1% of log lines.\n",
      "Processed 80.0% of log lines.\n",
      "Processed 81.0% of log lines.\n",
      "Processed 82.0% of log lines.\n",
      "Processed 83.0% of log lines.\n",
      "Processed 83.9% of log lines.\n",
      "Processed 84.9% of log lines.\n",
      "Processed 85.9% of log lines.\n",
      "Processed 86.9% of log lines.\n",
      "Processed 87.8% of log lines.\n",
      "Processed 88.8% of log lines.\n",
      "Processed 89.8% of log lines.\n",
      "Processed 90.8% of log lines.\n",
      "Processed 91.7% of log lines.\n",
      "Processed 92.7% of log lines.\n",
      "Processed 93.7% of log lines.\n",
      "Processed 94.7% of log lines.\n",
      "Processed 95.6% of log lines.\n",
      "Processed 96.6% of log lines.\n",
      "Processed 97.6% of log lines.\n",
      "Processed 98.6% of log lines.\n",
      "Processed 99.5% of log lines.\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:21.593582]\n"
     ]
    }
   ],
   "source": [
    "    log_file = \"BOI-JP-BBUEM1_CORE_20200608_0002_in_use.txt\"\n",
    "    base = os.path.basename(log_file)\n",
    "    file_name = os.path.splitext(base)[0]\n",
    "\n",
    "    log_format = '<timestamp>+<host> - <service> <thread_id> <message_id> <structured_data> - <log_level> <log_message>'\n",
    "    regex = [  # line 450\n",
    "        r'((Good)?ApplicationManagementService: \\w+(\\(\\)|) (\\- |)(\\[(.*?)\\]|))',\n",
    "        r\"\\{(.*?)\\}\",\n",
    "        r\"\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b\",  # ips\n",
    "        r\"[0-9A-Fa-f]{8}-?([0-9A-Fa-f]{4}-?){3}[0-9A-Fa-f]{12}\",  # guids\n",
    "        r\"(s|S)\\d{1,10}\",  # script id\n",
    "        r\"settingName=.*\",\n",
    "        r\"added settingInternalId.*\",  # settinginternal id\n",
    "        r\"http.*(,?\\]?\\s?)\",  # url\n",
    "        r\"(\\=com\\.).*$\",\n",
    "        r\"(?<=Public API).*$\",\n",
    "        r\"(?<=Checking sorted filter chain:).*$\",\n",
    "        r\"(?<=PostUpgradeStartupInitializer::).*$\",\n",
    "        r\"(?<=RECONCILIATION BATCH:).*$\",\n",
    "        r\"(?<=BB2FA for BlackBerry).*$\",\n",
    "        r\"(?<=Creating filter chain:).*$\",\n",
    "        r\"(?<=>>>SIS Snapin ).*$\",\n",
    "        r\"(?<=Snapin discovery:).*$\",\n",
    "        r\"(?<=Move system App To PermStore).*$\",\n",
    "        r\"(?<=getInvocationRegistrationRequest \\- ).*$\",\n",
    "        r\"(?<=Exchange\\[).*$\",\n",
    "        r\"(?<=TenantSyncTask:).*$\",\n",
    "        r\"(?<=Snapin \\[).*$ found in database already, not adding\",\n",
    "    ]\n",
    "\n",
    "    st = 0.5\n",
    "    depth = 4\n",
    "    parser = Parser(log_format=log_format, depth=depth, st=st, rex=regex)\n",
    "    parsed_log_file = parser.parse(log_file)\n",
    "    parsed_log_file.to_csv(\"parsed_%s.csv\" % file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
